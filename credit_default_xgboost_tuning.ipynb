{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Default Prediction with XGBoost and Hyperparameter Tuning\n",
    "\n",
    "This notebook covers a full pipeline:\n",
    "- Load Lending Club dataset from Kaggle\n",
    "- Data preprocessing\n",
    "- Model selection: XGBoost for binary classification\n",
    "- Bayesian Optimization for hyperparameter tuning\n",
    "- Model evaluation using accuracy, precision, recall, F1-score, AUC-ROC\n",
    "- Confusion matrix visualization\n",
    "\n",
    "**Note:** Data is loaded directly from Kaggle using the kaggle API. No local `data/` folder is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if running in a fresh environment)\n",
    "!pip install xgboost scikit-learn bayesian-optimization kagglehub matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix)\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Lending Club dataset from Kaggle using kagglehub\n",
    "path = kagglehub.dataset_download(\"wordsforthewise/lending-club\")\n",
    "print(\"Dataset files downloaded at:\", path)\n",
    "\n",
    "# Load the credit data CSV (adjust filename if different)\n",
    "file_path = f\"{path}/lending_club_loan_two.csv\"  # example filename, check actual\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview and Preprocessing\n",
    "- Check for missing values\n",
    "- Select relevant features\n",
    "- Encode categorical variables\n",
    "- Define target variable (default or not)\n",
    "- Split data into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "print(df.head())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, let's select a few numerical and categorical features\n",
    "features = [\"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\", \"dti\", \"open_acc\", \"revol_bal\", \"total_acc\", \"emp_length\"]\n",
    "target = \"loan_status\"  # Assuming this contains 'Fully Paid' vs 'Charged Off' or similar\n",
    "\n",
    "# Filter dataset\n",
    "df = df[features + [target]].dropna()\n",
    "\n",
    "# Convert target to binary: 1 if default, 0 if fully paid\n",
    "df[\"target\"] = df[target].apply(lambda x: 1 if x.lower() != \"fully paid\" else 0)\n",
    "\n",
    "# Encode employment length categorical feature (example encoding)\n",
    "def emp_length_to_int(emp):\n",
    "    if pd.isnull(emp):\n",
    "        return 0\n",
    "    if emp == '< 1 year':\n",
    "        return 0\n",
    "    if emp == '10+ years':\n",
    "        return 10\n",
    "    try:\n",
    "        return int(emp.split()[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['emp_length'] = df['emp_length'].apply(emp_length_to_int)\n",
    "\n",
    "# Define X, y\n",
    "X = df[features].copy()\n",
    "X['emp_length'] = df['emp_length']\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: XGBoost\n",
    "XGBoost is chosen for binary classification due to its strong performance on tabular data and multiple tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate model given hyperparameters\n",
    "def xgb_evaluate(max_depth, learning_rate, n_estimators, gamma, min_child_weight, subsample, colsample_bytree):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'gamma': gamma,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'use_label_encoder': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_train)[:,1]\n",
    "    auc = roc_auc_score(y_train, preds)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for Hyperparameter Tuning\n",
    "Optimize for highest AUC on training set (with validation later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'n_estimators': (50, 300),\n",
    "    'gamma': (0, 5),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1),\n",
    "    'colsample_bytree': (0.5, 1)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_evaluate,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best params\n",
    "best_params = optimizer.max['params']\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['use_label_encoder'] = False\n",
    "best_params['eval_metric'] = 'auc'\n",
    "best_params['random_state'] = 42\n",
    "\n",
    "# Train model\n",
    "final_model = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Set with Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"No Default\", \"Default\"], yticklabels=[\"No Default\", \"Default\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Data loaded directly from Kaggle dataset `wordsforthewise/lending-club`.\n",
    "- Task: binary classification to predict credit default.\n",
    "- Model: XGBoost with Bayesian hyperparameter optimization.\n",
    "- Evaluation using multiple metrics: accuracy, precision, recall, F1, AUC-ROC.\n",
    "- Confusion matrix plotted for error analysis.\n",
    "\n",
    "## Next Steps\n",
    "- Improve feature engineering.\n",
    "- Use cross-validation for more robust evaluation.\n",
    "- Experiment with other models and ensembles.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
